"""
16time_allnode_draw.py

ç”Ÿæˆæ—¶ç©ºç‰¹å¾å›¾ï¼Œç­›é€‰æŒ‡å®šæ—¶é—´çš„æ•°æ®ï¼Œç»™æ¯ä¸ªèŠ‚ç‚¹å¢åŠ ç›¸åº”çš„å±æ€§

è¾“å…¥:
    - plots/buffer/d2trajectory_10_Buf.shp: åŸºç¡€ shapefile
    - data/draw/d210191000/d210291000_lane_node_stats.csv: æ—¶ç©ºç»Ÿè®¡æ•°æ®

è¾“å‡º:
    - plots/inference/time_allnode/439/true/d210291000_lane_node_stats_439.shp
"""

import os
import pandas as pd
import geopandas as gpd
from shapefile_utils import read_shapefile_with_fallback


def main():
    # ========== é…ç½®å‚æ•° ==========
    TARGET_TIME = 142  # è¦ç­›é€‰çš„æ—¶é—´å€¼
    # ==============================
    
    # è·å–é¡¹ç›®æ ¹ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(script_dir)
    
    # è¾“å…¥æ–‡ä»¶è·¯å¾„
    base_shp = os.path.join(project_root, "plots/buffer/d2trajectory_10_Buf.shp")
    csv_file = os.path.join(project_root, "data/draw/d210191000/st_idw_results.csv")
    
    # è¾“å‡ºç›®å½•
    output_dir = os.path.join(project_root, "plots/inference/time_allnode")
    os.makedirs(output_dir, exist_ok=True)
    
    # è¯»å–åŸºç¡€ shapefile
    print("ğŸ“¦ æ­£åœ¨è¯»å–åŸºç¡€ Shapefile...")
    gdf_base = read_shapefile_with_fallback(base_shp, verbose=True)
    print(f"âœ… å…±åŠ è½½ {len(gdf_base)} ä¸ªè¦ç´ ")
    print(f"ğŸ“‹ Shapefile åˆ—å: {list(gdf_base.columns)}")
    
    # ç¡®ä¿ FID_ å­—æ®µå­˜åœ¨
    if 'FID_' not in gdf_base.columns:
        print(f"âŒ é”™è¯¯: Shapefile ä¸­æœªæ‰¾åˆ° 'FID_' å­—æ®µ")
        print(f"   å¯ç”¨å­—æ®µ: {list(gdf_base.columns)}")
        return
    
    # è¯»å– CSV æ•°æ®
    print("\nğŸ“Š æ­£åœ¨è¯»å– CSV æ•°æ®...")
    df = pd.read_csv(csv_file)
    print(f"âœ… å…±è¯»å– {len(df)} è¡Œæ•°æ®")
    print(f"ğŸ“‹ CSV åˆ—å: {list(df.columns)}")
    
    # ç­›é€‰ time åˆ—ä¸º TARGET_TIME çš„è¡Œ
    print(f"\nğŸ” ç­›é€‰ time={TARGET_TIME} çš„è¡Œ...")
    df = df[df['time'] == TARGET_TIME].copy()
    print(f"âœ… ç­›é€‰åå…± {len(df)} è¡Œæ•°æ®")
    
    # å¤„ç†é‡å¤çš„ node_idï¼Œä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„è¡Œ
    if df.duplicated(subset=['node_id']).any():
        duplicate_count = df.duplicated(subset=['node_id']).sum()
        print(f"âš ï¸ å‘ç° {duplicate_count} ä¸ªé‡å¤çš„ node_idï¼Œå°†ä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„è¡Œ")
        df = df.drop_duplicates(subset=['node_id'], keep='first')
        print(f"âœ… å»é‡åå…± {len(df)} è¡Œæ•°æ®")
    
    print(f"\n{'='*60}")
    print(f"ğŸ”§ ç”Ÿæˆ shapefile")
    print(f"{'='*60}")
    
    # å¤åˆ¶åŸºç¡€ GeoDataFrame
    gdf_result = gdf_base.copy()
    
    # è·å– CSV ä¸­çš„æ‰€æœ‰åˆ—ï¼ˆé™¤äº† node_idï¼‰
    data_columns = [col for col in df.columns if col != 'node_id']
    print(f"ğŸ“‹ è¦æ·»åŠ çš„æ•°æ®åˆ—: {data_columns}")
    
    # ä¸ºæ¯ä¸ªæ•°æ®åˆ—åœ¨ GeoDataFrame ä¸­åˆå§‹åŒ–ä¸º -1
    for col in data_columns:
        # åˆå§‹åŒ–ä¸º -1ï¼Œè¡¨ç¤ºç©ºå€¼
        gdf_result[col] = -1.0
    
    # éå† CSV ä¸­çš„æ¯ä¸€è¡Œï¼Œæ ¹æ® node_id åŒ¹é… FID_
    matched_count = 0
    unmatched_nodes = []
    
    for idx, row in df.iterrows():
        node_id = int(row['node_id'])
        
        # åœ¨ GeoDataFrame ä¸­æŸ¥æ‰¾åŒ¹é…çš„ FID_
        mask = gdf_result['FID_'] == node_id
        
        if mask.any():
            # å°†è¯¥è¡Œçš„æ‰€æœ‰æ•°æ®åˆ—æ·»åŠ åˆ°å¯¹åº”çš„ FID_ï¼Œæœ€å¤šä¿ç•™4ä½å°æ•°
            for col in data_columns:
                value = row[col]
                # å¦‚æœæ˜¯æ•°å€¼ç±»å‹ï¼Œæœ€å¤šä¿ç•™4ä½å°æ•°ï¼ˆä¸è¶³4ä½ä¿æŒåŸæ ·ï¼‰
                if pd.notna(value) and isinstance(value, (int, float)):
                    # å…ˆè½¬ä¸ºæµ®ç‚¹æ•°å¹¶å››èˆäº”å…¥åˆ°4ä½å°æ•°
                    rounded_value = round(float(value), 4)
                    # ä¿æŒä¸ºæµ®ç‚¹æ•°ç±»å‹ï¼Œå³ä½¿æ˜¯æ•´æ•°å€¼
                    gdf_result.loc[mask, col] = float(rounded_value)
                else:
                    # å¦‚æœæ˜¯ç©ºå€¼ï¼Œè®¾ç½®ä¸º -1
                    gdf_result.loc[mask, col] = -1.0
            matched_count += 1
        else:
            unmatched_nodes.append(node_id)
    
    print(f"âœ… æˆåŠŸåŒ¹é… {matched_count}/{len(df)} ä¸ªèŠ‚ç‚¹")
    if unmatched_nodes:
        print(f"âš ï¸ æœªåŒ¹é…çš„ node_id: {unmatched_nodes[:10]}{'...' if len(unmatched_nodes) > 10 else ''}")
    
    # ç¡®ä¿æ‰€æœ‰æ•°æ®åˆ—éƒ½æ˜¯æµ®ç‚¹æ•°ç±»å‹ï¼Œå¹¶å°† NaN æ›¿æ¢ä¸º -1
    print(f"\nğŸ”§ æ­£åœ¨è½¬æ¢æ•°æ®ç±»å‹...")
    for col in data_columns:
        gdf_result[col] = pd.to_numeric(gdf_result[col], errors='coerce')
        # å°† NaN æ›¿æ¢ä¸º -1
        gdf_result[col] = gdf_result[col].fillna(-1.0)
    print(f"âœ… æ•°æ®ç±»å‹è½¬æ¢å®Œæˆ")
    
    # æ‰“å°æ•°æ®ç±»å‹ä¿¡æ¯
    print(f"\nğŸ“Š å­—æ®µæ•°æ®ç±»å‹:")
    for col in data_columns:
        print(f"   {col}: {gdf_result[col].dtype}")
    
    # ä¿å­˜ä¸ºæ–°çš„ shapefile
    # ä» CSV æ–‡ä»¶åä¸­æå–åŸºç¡€åç§°ï¼ˆä¸å«æ‰©å±•åï¼‰
    csv_basename = os.path.splitext(os.path.basename(csv_file))[0]
    output_filename = f"{csv_basename}_{TARGET_TIME}.shp"
    output_path = os.path.join(output_dir, output_filename)
    print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜åˆ°: {output_path}")
    gdf_result.to_file(output_path, driver='ESRI Shapefile')
    print(f"âœ… æˆåŠŸä¿å­˜ {output_filename}")
    
    print(f"\n{'='*60}")
    print("ğŸ‰ Shapefile åˆ›å»ºå®Œæˆï¼")
    print(f"{'='*60}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_path}")


if __name__ == "__main__":
    main()
